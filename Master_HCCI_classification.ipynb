{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN  -  Accuracy estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 14 candidates, totalling 140 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=19)]: Done  12 tasks      | elapsed:    4.9s\n",
      "[Parallel(n_jobs=19)]: Done 140 out of 140 | elapsed:   17.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.9\n",
      "Fitting 10 folds for each of 14 candidates, totalling 140 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=19)]: Done  12 tasks      | elapsed:    5.2s\n",
      "[Parallel(n_jobs=19)]: Done 140 out of 140 | elapsed:   16.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.897115384615\n",
      "Fitting 10 folds for each of 14 candidates, totalling 140 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=19)]: Done  12 tasks      | elapsed:    5.3s\n",
      "[Parallel(n_jobs=19)]: Done 140 out of 140 | elapsed:   16.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.876923076923\n",
      "Fitting 10 folds for each of 14 candidates, totalling 140 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=19)]: Done  12 tasks      | elapsed:    5.4s\n",
      "[Parallel(n_jobs=19)]: Done 140 out of 140 | elapsed:   16.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.880769230769\n",
      "Fitting 10 folds for each of 14 candidates, totalling 140 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=19)]: Done  12 tasks      | elapsed:    5.8s\n",
      "[Parallel(n_jobs=19)]: Done 140 out of 140 | elapsed:   16.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.904807692308\n",
      "Fitting 10 folds for each of 14 candidates, totalling 140 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=19)]: Done  12 tasks      | elapsed:    5.4s\n",
      "[Parallel(n_jobs=19)]: Done 140 out of 140 | elapsed:   17.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.886429258903\n",
      "Fitting 10 folds for each of 14 candidates, totalling 140 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=19)]: Done  12 tasks      | elapsed:    5.1s\n",
      "[Parallel(n_jobs=19)]: Done 140 out of 140 | elapsed:   15.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.898941289702\n",
      "Fitting 10 folds for each of 14 candidates, totalling 140 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=19)]: Done  12 tasks      | elapsed:    5.2s\n",
      "[Parallel(n_jobs=19)]: Done 140 out of 140 | elapsed:   15.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.881616939365\n",
      "Fitting 10 folds for each of 14 candidates, totalling 140 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=19)]: Done  12 tasks      | elapsed:    5.2s\n",
      "[Parallel(n_jobs=19)]: Done 140 out of 140 | elapsed:   16.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.881616939365\n",
      "Fitting 10 folds for each of 14 candidates, totalling 140 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=19)]: Done  12 tasks      | elapsed:    5.4s\n",
      "[Parallel(n_jobs=19)]: Done 140 out of 140 | elapsed:   16.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.876804619827\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import metrics\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_excel('HCCI CFR data.xlsx', sheet_name = 'Data Compiled', index_col=0)\n",
    "target = df['Output']\n",
    "features = df[df.columns[0:9]]\n",
    "\n",
    "# Define search space\n",
    "n_neighbors = [2,3,5,6,7,9,10,11,13,14,15,17,18,19]\n",
    "\n",
    "# Setup the grid to be searched over\n",
    "param_grid = dict(n_neighbors=n_neighbors)\n",
    "\n",
    "# Define outer folds\n",
    "kFolds = KFold(n_splits=10, shuffle=True, random_state=1).split(X=features.values, y=target.values)\n",
    "\n",
    "# Define inner folds\n",
    "grid_search = GridSearchCV(KNeighborsClassifier(weights='uniform'), param_grid, cv=KFold(n_splits=10, shuffle=True, random_state=1),\n",
    "                           n_jobs=19, verbose=1, scoring='precision_micro')\n",
    "\n",
    "# Open results file and write out headers\n",
    "out_file = open(\"grid_search_KNN.csv\", 'w')\n",
    "wr = csv.writer(out_file, dialect='excel')\n",
    "headers = ['neighbours', 'micro_precision']\n",
    "wr.writerow(headers)\n",
    "out_file.flush()\n",
    "\n",
    "KNN_file = open(\"KNN_results.csv\", 'w',newline='')\n",
    "wrr = csv.writer(KNN_file, dialect='excel')\n",
    "headers = ['Actual', 'Predicted']\n",
    "wrr.writerow(headers)\n",
    "KNN_file.flush()\n",
    "\n",
    "\n",
    "for index_train, index_test in kFolds:\n",
    "    \n",
    "    # Get train and test splits\n",
    "    x_train, x_test = features.iloc[index_train].values, features.iloc[index_test].values\n",
    "    y_train, y_test = target.iloc[index_train].values, target.iloc[index_test].values\n",
    "\n",
    "    # Apply min max normalization\n",
    "    scaler = MinMaxScaler().fit(x_train)\n",
    "    x_train = scaler.transform(x_train)\n",
    "    x_test = scaler.transform(x_test)\n",
    "\n",
    "    # Fit\n",
    "    grid_search.fit(x_train, y_train)\n",
    "\n",
    "    # Get best params\n",
    "    best_params = grid_search.best_params_\n",
    "    \n",
    "    ##Testing\n",
    "    knn = KNeighborsClassifier(n_neighbors=best_params['n_neighbors'], weights='uniform')\n",
    "    knn.fit(x_train, y_train)\n",
    "    Y_pred = knn.predict(x_test)\n",
    "    print(\"precision:\",metrics.precision_score(Y_pred, y_test, average='micro'))\n",
    "\n",
    "    # Write results\n",
    "    row = [best_params['n_neighbors'], metrics.precision_score(Y_pred, y_test, average='micro')]\n",
    "    wr.writerow(row)\n",
    "    out_file.flush()\n",
    "    \n",
    "    # Write results\n",
    "    for i in range(len(y_test)):\n",
    "        row = (y_test[i], Y_pred[i])\n",
    "        wrr.writerow(row)\n",
    "        KNN_file.flush()\n",
    "    \n",
    "out_file.close()\n",
    "KNN_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM  -  Accuracy estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 5 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=19)]: Done  12 tasks      | elapsed:   11.1s\n",
      "[Parallel(n_jobs=19)]: Done  50 out of  50 | elapsed:   57.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 1000}\n",
      "0.919865319865\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import pickle\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import metrics\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_excel('HCCI CFR data.xlsx', sheet_name = 'Data Compiled', index_col=0)\n",
    "target = df['Output']\n",
    "# features = df[df.columns[0:7]]\n",
    "#for sensitivity analysis remove a feature and run this block -- repeat this for all the seven features\n",
    "features = df[['RON','S','Fuel rate','O2','Intake Temperature','Intake Pressure','Compression ratio']]\n",
    "# Define search space\n",
    "C = [1, 10, 100, 1000, 10000]\n",
    "\n",
    "# Setup the grid to be searched over\n",
    "param_grid = dict(C=C)\n",
    "\n",
    "####Test sets accuracy \n",
    "\n",
    "# Define outer folds\n",
    "kFolds = KFold(n_splits=10, shuffle=True, random_state=1).split(X=features.values, y=target.values)\n",
    "\n",
    "# Define inner folds\n",
    "grid_search = GridSearchCV(SVC(gamma='auto', kernel = 'rbf'), param_grid, cv=KFold(n_splits=10, shuffle=True, random_state=1),\n",
    "                           n_jobs=19, verbose=1, scoring='precision_micro')\n",
    "\n",
    "# Open results file and write out headers\n",
    "out_file = open(\"grid_search_SVM.csv\", 'w')\n",
    "wr = csv.writer(out_file, dialect='excel')\n",
    "headers = ['C', 'micro_precision']\n",
    "wr.writerow(headers)\n",
    "out_file.flush()\n",
    "\n",
    "SVM_file = open(\"SVM_results.csv\", 'w',newline='')\n",
    "wrr = csv.writer(SVM_file, dialect='excel')\n",
    "headers = ['Actual', 'Predicted']\n",
    "wrr.writerow(headers)\n",
    "SVM_file.flush()\n",
    "\n",
    "for index_train, index_test in kFolds:\n",
    "    # Get train and test splits\n",
    "    x_train, x_test = features.iloc[index_train].values, features.iloc[index_test].values\n",
    "    y_train, y_test = target.iloc[index_train].values, target.iloc[index_test].values\n",
    "\n",
    "    # Apply min max normalization\n",
    "    scaler = MinMaxScaler().fit(x_train)\n",
    "    x_train = scaler.transform(x_train)\n",
    "    x_test = scaler.transform(x_test)\n",
    "\n",
    "    # Fit\n",
    "    grid_search.fit(x_train, y_train)\n",
    "\n",
    "    # Get best params\n",
    "    best_params = grid_search.best_params_\n",
    "    \n",
    "    ##Testing\n",
    "    svm = SVC(gamma='auto', kernel = 'rbf', C=best_params['C'])\n",
    "    svm.fit(x_train, y_train)\n",
    "    Y_pred = svm.predict(x_test)\n",
    "    print(\"precision:\",metrics.precision_score(Y_pred, y_test, average='micro'))\n",
    "\n",
    "    # Write results\n",
    "    row = [best_params['C'], metrics.precision_score(Y_pred, y_test, average='micro')]\n",
    "    wr.writerow(row)\n",
    "    out_file.flush()\n",
    "\n",
    "    # Write results\n",
    "    for i in range(len(y_test)):\n",
    "        row = (y_test[i], Y_pred[i])\n",
    "        wrr.writerow(row)\n",
    "        SVM_file.flush()\n",
    "\n",
    "out_file.close()\n",
    "SVM_file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM  -  Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import pickle\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import metrics\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_excel('HCCI CFR data.xlsx', sheet_name = 'Data Compiled', index_col=0)\n",
    "target = df['Output']\n",
    "# features = df[df.columns[0:9]]\n",
    "features = df[['RON','S','Fuel rate','O2','Intake Temperature','Intake Pressure','Compression ratio']]\n",
    "# features = df[['RON','S','Fuel rate','O2','Intake Temperature','Intake Pressure','Compression ratio', 'M.W', 'LHV(KJ/kg)']]\n",
    "# Define search space\n",
    "C = [1, 10, 100, 1000, 10000]\n",
    "\n",
    "# Setup the grid to be searched over\n",
    "param_grid = dict(C=C)\n",
    "\n",
    "\n",
    "#####Save Final Model\n",
    "\n",
    "# Define grid search\n",
    "grid_search = GridSearchCV(SVC(kernel='rbf', gamma='auto'), param_grid, cv=KFold(n_splits=10, shuffle=True, random_state=1),\n",
    "                           n_jobs=19, verbose=1, scoring='precision_micro')\n",
    "\n",
    "# Split data in to features and target\n",
    "x_train = features.values\n",
    "y_train = target.values\n",
    "\n",
    "# Apply min max normalization\n",
    "scaler = MinMaxScaler().fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "\n",
    "# Find best parameters\n",
    "grid_search.fit(x_train, y_train)\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_score_)\n",
    "\n",
    "# Retrain model with best parameters found from grid search\n",
    "best_params = grid_search.best_params_\n",
    "model = SVC(kernel='rbf', gamma='auto', C=best_params['C'])\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# save the model\n",
    "filename = 'final_SVR_model.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data for contour diagrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import csv\n",
    "from keras.models import load_model\n",
    "\n",
    "# Load SVR model\n",
    "filename = 'final_SVR_model.sav'\n",
    "model = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "out_file = open(\"Counter_diagram_data.csv\", 'w')\n",
    "wr = csv.writer(out_file, dialect='excel', lineterminator = '\\n')\n",
    "headers = ['RON','S','Fuel rate','O2','Intake Temperature','Intake Pressure','CR','MW','LHV','Output']\n",
    "wr.writerow(headers)\n",
    "out_file.flush()\n",
    "\n",
    "\n",
    "for i in range(0,101,1):\n",
    "    for j in range (0,101,1):\n",
    "        inp = [[i/100,0.34,j/100,1,0.3333,0,0]]\n",
    "        res = model.predict(inp)\n",
    "        #Write results\n",
    "        row = [inp[0][0],inp[0][1],inp[0][2],inp[0][3],inp[0][4],inp[0][5],inp[0][6],res[0]]\n",
    "        wr.writerow(row)\n",
    "        out_file.flush()\n",
    "\n",
    "for i in range(0,101,1):\n",
    "    for j in range (0,101,1):\n",
    "        inp = [[i/100,0.34,j/100,1,0.3333,0,0.3333]]\n",
    "        res = model.predict(inp)\n",
    "        #Write results\n",
    "        row = [inp[0][0],inp[0][1],inp[0][2],inp[0][3],inp[0][4],inp[0][5],inp[0][6],res[0]]\n",
    "        wr.writerow(row)\n",
    "        out_file.flush()\n",
    "\n",
    "for i in range(0,101,1):\n",
    "    for j in range (0,101,1):\n",
    "        inp = [[i/100,0.34,j/100,1,0.3333,0,0.6666]]\n",
    "        res = model.predict(inp)\n",
    "        #Write results\n",
    "        row = [inp[0][0],inp[0][1],inp[0][2],inp[0][3],inp[0][4],inp[0][5],inp[0][6],res[0]]\n",
    "        wr.writerow(row)\n",
    "        out_file.flush()\n",
    "\n",
    "for i in range(0,101,1):\n",
    "    for j in range (0,101,1):\n",
    "        inp = [[i/100,0.34,j/100,1,0.3333,0,1]]\n",
    "        res = model.predict(inp)\n",
    "        #Write results\n",
    "        row = [inp[0][0],inp[0][1],inp[0][2],inp[0][3],inp[0][4],inp[0][5],inp[0][6],res[0]]\n",
    "        wr.writerow(row)\n",
    "        out_file.flush()\n",
    "\n",
    "\n",
    "for i in range(0,101,1):\n",
    "    for j in range (0,101,1):\n",
    "        inp = [[i/100,0.34,j/100,1,0.3333,0.2,0]]\n",
    "        res = model.predict(inp)\n",
    "        #Write results\n",
    "        row = [inp[0][0],inp[0][1],inp[0][2],inp[0][3],inp[0][4],inp[0][5],inp[0][6],res[0]]\n",
    "        wr.writerow(row)\n",
    "        out_file.flush()\n",
    "\n",
    "for i in range(0,101,1):\n",
    "    for j in range (0,101,1):\n",
    "        inp = [[i/100,0.34,j/100,1,0.3333,0.2,0.3333]]\n",
    "        res = model.predict(inp)\n",
    "        #Write results\n",
    "        row = [inp[0][0],inp[0][1],inp[0][2],inp[0][3],inp[0][4],inp[0][5],inp[0][6],res[0]]\n",
    "        wr.writerow(row)\n",
    "        out_file.flush()\n",
    "\n",
    "for i in range(0,101,1):\n",
    "    for j in range (0,101,1):\n",
    "        inp = [[i/100,0.34,j/100,1,0.3333,0.2,0.6666]]\n",
    "        res = model.predict(inp)\n",
    "        #Write results\n",
    "        row = [inp[0][0],inp[0][1],inp[0][2],inp[0][3],inp[0][4],inp[0][5],inp[0][6],res[0]]\n",
    "        wr.writerow(row)\n",
    "        out_file.flush()\n",
    "\n",
    "for i in range(0,101,1):\n",
    "    for j in range (0,101,1):\n",
    "        inp = [[i/100,0.34,j/100,1,0.3333,0.2,1]]\n",
    "        res = model.predict(inp)\n",
    "        #Write results\n",
    "        row = [inp[0][0],inp[0][1],inp[0][2],inp[0][3],inp[0][4],inp[0][5],inp[0][6],res[0]]\n",
    "        wr.writerow(row)\n",
    "        out_file.flush()\n",
    "\n",
    "out_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
